{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0G-cp1DlkmR"
      },
      "source": [
        "![Practicum AI Logo image](https://github.com/PracticumAI/deep_learning_pt-lightning/blob/main/images/practicum_ai_logo.png?raw=1) <img src='https://github.com/PracticumAI/deep_learning_pt-lightning/blob/main/images/practicumai_deep_learning.png?raw=1' alt='Practicum AI: Deep Learning Foundations icon' align='right' width=50>\n",
        "\n",
        "***\n",
        "# *Practicum AI:* Deep Learning - MNIST Classifier\n",
        "\n",
        "This exercise adapted from Baig et al. (2020) <i>The Deep Learning Workshop</i> from <a href=\"https://www.packtpub.com/product/the-deep-learning-workshop/9781839219856\">Packt Publishers</a> (Exercise 2.07, page 92).\n",
        "\n",
        "## Amelia's AI Adventure Continues...\n",
        "\n",
        "<img alt=\"A cartoon of Dr. Amelia's dog looking at a computer with a stack of papers next to it showing some handwritten digits.\" src=\"https://github.com/PracticumAI/deep_learning_pt-lightning/blob/main/images/Amelias_Dog_MNIST.jpg?raw=1\" padding=20 align=\"right\" width=250>Amelia and her nutrition studies are back! After her adventures with image recognition and binary classification, she's curious to dive deeper.\n",
        "\n",
        "While Amelia's data collection process is working for most participants in her study, some do not like using the phone application to submit their survey responses. They keep sending in handwritten responses. Realizing that the data from these study participants is still vital to her research, Dr. Amelia is now looking to automate entering these responses using a program to read the numbers that make up the survey responses.\n",
        "\n",
        "Again, Amelia decides to start with the basics: recognizing handwritten numbers. That's where the MNIST dataset comes in. With its vast collection of handwritten digits, it's the perfect training ground for Amelia's next AI venture.\n",
        "\n",
        "**Note:** The cartoon of Dr Amelia's dog was generated with AI's assistance.\n",
        "\n",
        "Training a model on the MNIST dataset is often considered the \"Hello world!\" of AI. It is a commonly used first introduction to image recognition with deep learning.\n",
        "\n",
        "\n",
        "![AI Application Development Pathway model](https://github.com/PracticumAI/deep_learning_2_draft/blob/main/M3-AppDev.00_00_22_23.Still001.png?raw=true)\n",
        "\n",
        " >&#128221; While you're going through this notebook, see if you can figure out which steps here are associated with each of the steps of the Development Pathway.\n",
        "\n",
        "## MNIST Handwritten Digit Classification Dataset\n",
        "\n",
        "The [MNIST](http://yann.lecun.com/exdb/mnist/) (Modified National Institute of Standards and Technology) training dataset contains 60,000 28√ó28 pixel grayscale images of handwritten single digits between 0 and 9, with an additional 10,000 images available for validation.\n",
        "\n",
        "The MNIST dataset is frequently used in machine learning research and has become a standard benchmark for image classification models. Top-performing models often achieve a classification accuracy above 99%, with an error rate between 0.4% and 0.2% on the hold-out validation dataset.\n",
        "\n",
        "In this exercise, you will implement a deep neural network (multi-layer) capable of classifying these images of handwritten digits into one of 10 classes.\n",
        "\n",
        "Amelia knows that to start any AI project, she'll need the right tools. She begins by importing the necessary libraries to set the stage for her digit-reading neural network.\n",
        "\n",
        "## 1. Import libraries\n",
        "\n",
        "Import the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nunfKsa3Rx-E",
        "outputId": "9c19dbcd-184e-41f2-8ea5-0e08034007eb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.6.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (2.9.0+cu126)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>5.4 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (6.0.3)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2025.3.0)\n",
            "Collecting torchmetrics>0.7.0 (from pytorch-lightning)\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (25.0)\n",
            "Requirement already satisfied: typing-extensions>4.5.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (4.15.0)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.13.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.5.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics>0.7.0->pytorch-lightning) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.22.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.1.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.3)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.11)\n",
            "Downloading pytorch_lightning-2.6.0-py3-none-any.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m849.5/849.5 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lightning-utilities, torchmetrics, pytorch-lightning\n",
            "Successfully installed lightning-utilities-0.15.2 pytorch-lightning-2.6.0 torchmetrics-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2VuBLwQLlkmS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt  # Import the matplotlib library for plotting and visualization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiD7g6a6RfUZ"
      },
      "source": [
        "\n",
        "While the GPU does most of the calculations in training AI models, the CPU of the computer server is responsible for loading images from disk, doing any transformations and sending the data to the GPU. PyTorch takes care of all of this and takes care of doing this in parallel. For maximum GPU performance, multiple cores are needed to constantly feed data to the GPU. The number of workers (num_workers) argument controls how many parallel tasks should be running to load data.\n",
        "\n",
        "The code block below will detect if your notebook is running in a Slurm job by checking for Slurm environment variables that specify how many CPUs are available and using that information to set the number of workers, if possible. Otherwise, the code checks for the number of cores on your computer and uses that value. To manually set the number of workers, change the first line by adding the value you want to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xl8DJGqmRfUZ",
        "outputId": "ea22ed1c-ba74-4cfb-a90a-dee409462218"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 2 workers for data loading.\n"
          ]
        }
      ],
      "source": [
        "# Set the number of workers to use for data loading\n",
        "num_workers = None  # To manually set the number of workers, change this to an integer\n",
        "\n",
        "if num_workers is None:\n",
        "    # If Slurm is being used, set the number of workers to a Slurm-provided value.\n",
        "    # If Slurm is not being used, set the number of workers to the number of available CPU cores.\n",
        "    if os.getenv(\"SLURM_CPUS_PER_TASK\") is not None:\n",
        "        num_workers = int(os.getenv(\"SLURM_CPUS_PER_TASK\"))\n",
        "    elif os.getenv(\"SLURM_NTASKS_PER_NODE\") is not None:\n",
        "        num_workers = int(os.getenv(\"SLURM_NTASKS_PER_NODE\"))\n",
        "    elif os.getenv(\"SLURM_NTASKS\") is not None:\n",
        "        num_workers = int(os.getenv(\"SLURM_NTASKS\"))\n",
        "    else:\n",
        "        num_workers = os.cpu_count()\n",
        "\n",
        "print(f\"Using {num_workers} workers for data loading.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTVTtpTilkmS"
      },
      "source": [
        "## 2. Load the MNIST dataset\n",
        "\n",
        "Amelia will need to import the MNIST dataset from PyTorch's [torchvision.datasets module](https://pytorch.org/vision/stable/datasets.html#mnist). The `train_features` and `val_features` variables contain the training and validation images, while `train_labels` and `val_labels` contain the corresponding labels for each item in those datasets.  \n",
        "\n",
        "Notice that when training with MNIST data, the normalization is different than with the Imagenet data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1K6B_mRUlkmS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd6e4a27-d022-4d96-9788-0f01c4f3130c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9.91M/9.91M [00:00<00:00, 17.8MB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28.9k/28.9k [00:00<00:00, 465kB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.65M/1.65M [00:00<00:00, 4.49MB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.54k/4.54k [00:00<00:00, 4.21MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Define transforms to convert PIL images to tensors and normalize\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,)),  # MNIST mean and std\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Load the MNIST dataset from torchvision\n",
        "train_dataset = torchvision.datasets.MNIST(\n",
        "    root=\"./data\", train=True, download=True, transform=transform\n",
        ")\n",
        "val_dataset = torchvision.datasets.MNIST(\n",
        "    root=\"./data\", train=False, download=True, transform=transform\n",
        ")\n",
        "\n",
        "# Extract features and labels for compatibility with visualization parts below\n",
        "train_features = train_dataset.data.numpy()\n",
        "train_labels = train_dataset.targets.numpy()\n",
        "val_features = val_dataset.data.numpy()\n",
        "val_labels = val_dataset.targets.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SeT9xkGlkmS"
      },
      "source": [
        "## 3. Visualize the data\n",
        "\n",
        "Before we start to work with data, it is always good to get a better idea of what we are working with.\n",
        "\n",
        "How many images do we have in our training and validation datasets?\n",
        "\n",
        "**Note**: We are using the un-transformed `train_features` here. Later, when we train the model, we will use `train_dataset` and a dataloader that will transform the images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SJo0QiYslkmS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ce85bcb-bcd4-4d82-86ec-f08f8f456987"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training images: 60000\n",
            "Validation images: 10000\n",
            "Image shape: (28, 28)\n"
          ]
        }
      ],
      "source": [
        "print(f\"Training images: {len(train_features)}\")\n",
        "print(f\"Validation images: {len(val_features)}\")\n",
        "print(f\"Image shape: {train_features[0].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JL4Pz1N6lkmT"
      },
      "source": [
        "Let's have a look at a random image. You can run this cell multiple times and get a different image each time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "21yyiVlClkmT",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 933
        },
        "outputId": "48307463-c2fc-410f-a9c4-4f354303be2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0 117 255 160   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0  65 242 254 252 165  40   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0  64 243 254 236 254 254 244 228 228 117   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   5 213 254 245  79 125 233 254 254 254 187   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0 120 254 254 118   0   0  37 254 254 224  31   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  55 236 254 198   5   0   0  85 254 254 152   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0 201 254 254  64   0   0   0 108 254 254  68   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0 120 251  92   1   0   0   9 211 254 203   3   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   2  17   0   0   0   0  52 254 254 153   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 121 254 253  70   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0  22 215 254 175   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0 115 254 254 123   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0 135 254 241  20   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0 223 254 207   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0  80 253 254 146   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0 147 254 247  53   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0 193 254 145   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0  58 250 254 177   7   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0 108 254 254 236  35   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0 161 254 254 133   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The true label for this image is a 7.\n"
          ]
        }
      ],
      "source": [
        "# Set line width for numpy array printing\n",
        "np.set_printoptions(linewidth=150)\n",
        "\n",
        "# Select a random number from train_features\n",
        "select = np.random.randint(0, len(train_features))\n",
        "\n",
        "# Print the image array - longer line length above should allow it to have all 28 rows in 1 line\n",
        "print(train_features[select])\n",
        "\n",
        "# Display the image as an actual image\n",
        "plt.imshow(train_features[select], cmap=\"gray\")\n",
        "plt.show()\n",
        "\n",
        "# Print the true label for the image from train_labels\n",
        "print(f\"The true label for this image is a {train_labels[select]}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7s3l-7ElkmT"
      },
      "source": [
        "The output of the cell above should help clarify how images are encoded in our data. Each pixel has a value from 0 (black) to 255 (white). Since our images are black and white, we only have one grid of pixels. For color images, we would have three: one for each color, red, green, blue.\n",
        "\n",
        "Our datasets have 60,000 images in the `train_features` and 10,000 images in the `val_features`. We will use these data as we move forward."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S47eIJZ_lkmT"
      },
      "source": [
        "## 4. Build the sequential model using PyTorch Lightning\n",
        "\n",
        "Now, the fun part begins! Amelia sets out to build her neural network. In the previous exercises, Amelia called a pre-trained model for image recognition and then built a single-layer network for her binary classifier. With her confidence high, she is going to create this model herself out of multiple layers. This approach gives her (and you!) the most control over the function of the model.\n",
        "\n",
        "Using PyTorch Lightning, we'll create a model class that encapsulates the neural network architecture and training logic. The model will have the following structure:\n",
        "\n",
        "* First, add a flattened layer to unroll the 28x28 pixel images into a single array of 784.\n",
        "* Add a dense hidden layer with 50 units (neurons) and ReLU (Rectified Linear Unit) activation function.\n",
        "   * The ReLU function will allow the model to capture non-linearity.\n",
        "* Add a second, dense hidden layer with 20 units and ReLU activation function.\n",
        "* Add a dense output layer with 10 units and the softmax activation function.\n",
        "   * We use ten neurons, each representing the digits 0-9.\n",
        "   * The softmax function ensures the output values are probabilities that sum to 1, making it suitable for classification.\n",
        "\n",
        "Here's a graphical view of what we are doing:\n",
        "\n",
        "![A diagram of the neural network being created. It shows the input 28X28 image being flattened into a 784 dimension array. That is the input. There are two hidden, fully connected layers with 50 and 20 neurons each. The final output layer has 10 neurons for the 10 classes in our model.](https://github.com/PracticumAI/deep_learning_pt-lightning/blob/main/images/MNIST_neural_network.png?raw=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "751EZdU_lkmT"
      },
      "outputs": [],
      "source": [
        "# Define our model with improved logging for epoch metrics only\n",
        "class MNISTClassifier(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Define the layers of the model\n",
        "        self.flatten = nn.Flatten()  # Flatten 28x28 to 784\n",
        "        self.fc1 = nn.Linear(784, 50)  # 784 inputs to 50 neurons\n",
        "        self.fc2 = nn.Linear(50, 20)  # 50 outputs to 20 neurons\n",
        "        self.fc3 = nn.Linear(20, 10)  # 20 to 10 output classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Define how the data flows through the layers of the model\n",
        "        # Also add in activation functions and other options\n",
        "        x = self.flatten(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.log_softmax(self.fc3(x), dim=1)  # Log softmax for NLLLoss\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # Define how the model is trained\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = F.nll_loss(y_hat, y)\n",
        "        preds = torch.argmax(y_hat, dim=1)\n",
        "        acc = (preds == y).float().mean()\n",
        "\n",
        "        # Log training metrics\n",
        "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log(\"train_acc\", acc, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = F.nll_loss(y_hat, y)\n",
        "        preds = torch.argmax(y_hat, dim=1)\n",
        "        acc = (preds == y).float().mean()\n",
        "\n",
        "        # Log validation metrics\n",
        "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log(\"val_acc\", acc, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        return {\"val_loss\": loss, \"val_acc\": acc}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # Define the optimizer and learning rate\n",
        "        return optim.Adam(self.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "# Instantiate the model\n",
        "model = MNISTClassifier()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKB6819UlkmT"
      },
      "source": [
        "## 5. Prepare the data loaders\n",
        "\n",
        "In PyTorch, we need to create data loaders to efficiently batch and iterate through our data during training. PyTorch Lightning works seamlessly with PyTorch's DataLoader.\n",
        "\n",
        "The `num_workers` variable was set above and controls the number of CPUs that will be used to load and process the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "tuL1IJ0ulkmT"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True,\n",
        "                        num_workers=num_workers)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False,\n",
        "                        num_workers=num_workers)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2sNR3VIlkmT"
      },
      "source": [
        "## 6. Inspect the model configuration using print\n",
        "\n",
        "Display a summary of the model's architecture, including the layers, their shapes, and the number of parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7g6WEAOLlkmT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ca58e3f-13be-412d-dc88-2cee076a4e4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNISTClassifier(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (fc1): Linear(in_features=784, out_features=50, bias=True)\n",
            "  (fc2): Linear(in_features=50, out_features=20, bias=True)\n",
            "  (fc3): Linear(in_features=20, out_features=10, bias=True)\n",
            ")\n",
            "\n",
            "Total parameters: 40480\n"
          ]
        }
      ],
      "source": [
        "print(model)\n",
        "\n",
        "# Count the total parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"\\nTotal parameters: {total_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oa4I-_zLlkmT"
      },
      "source": [
        "The model summary indicates that this model has 40,480 parameters (weights and biases). **Note**: If your model does not show `Total parameters: 40480`, double check your model was set up correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmhSAd3ylkmT"
      },
      "source": [
        "## 7. Train the model using PyTorch Lightning\n",
        "\n",
        "Now, train the model on the MNIST dataset using PyTorch Lightning's `Trainer`. We'll set the training to run for 10 epochs.\n",
        "\n",
        "Train the model using the training data:\n",
        "* `train_loader`: the DataLoader containing input images and labels\n",
        "* `max_epochs=10`: the number of times the model will cycle through the entire dataset\n",
        "\n",
        "```python\n",
        "# Create a PyTorch Lightning trainer\n",
        "trainer = Trainer(max_epochs=10, enable_progress_bar=True)\n",
        "\n",
        "# Train the model\n",
        "trainer.fit(model, train_loader, val_loader)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "oVa6WxzYlkmT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487,
          "referenced_widgets": [
            "b859785d54a143f6bfbb506c8120bfcf",
            "be127b5393ec48d6b4c4f3e0b1e96751"
          ]
        },
        "outputId": "530704bf-27d1-493d-aca8-df37b9a48ebb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "‚îè‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
              "‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35mName   \u001b[0m\u001b[1;35m \u001b[0m‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35mType   \u001b[0m\u001b[1;35m \u001b[0m‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35mFLOPs\u001b[0m\u001b[1;35m \u001b[0m‚îÉ\n",
              "‚î°‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
              "‚îÇ\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m‚îÇ flatten ‚îÇ Flatten ‚îÇ      0 ‚îÇ train ‚îÇ     0 ‚îÇ\n",
              "‚îÇ\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m‚îÇ fc1     ‚îÇ Linear  ‚îÇ 39.2 K ‚îÇ train ‚îÇ     0 ‚îÇ\n",
              "‚îÇ\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m‚îÇ fc2     ‚îÇ Linear  ‚îÇ  1.0 K ‚îÇ train ‚îÇ     0 ‚îÇ\n",
              "‚îÇ\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m‚îÇ fc3     ‚îÇ Linear  ‚îÇ    210 ‚îÇ train ‚îÇ     0 ‚îÇ\n",
              "‚îî‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
              "‚îÉ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>‚îÉ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name    </span>‚îÉ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type    </span>‚îÉ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>‚îÉ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>‚îÉ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> FLOPs </span>‚îÉ\n",
              "‚î°‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
              "‚îÇ<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>‚îÇ flatten ‚îÇ Flatten ‚îÇ      0 ‚îÇ train ‚îÇ     0 ‚îÇ\n",
              "‚îÇ<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>‚îÇ fc1     ‚îÇ Linear  ‚îÇ 39.2 K ‚îÇ train ‚îÇ     0 ‚îÇ\n",
              "‚îÇ<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>‚îÇ fc2     ‚îÇ Linear  ‚îÇ  1.0 K ‚îÇ train ‚îÇ     0 ‚îÇ\n",
              "‚îÇ<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>‚îÇ fc3     ‚îÇ Linear  ‚îÇ    210 ‚îÇ train ‚îÇ     0 ‚îÇ\n",
              "‚îî‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mTrainable params\u001b[0m: 40.5 K                                                                                           \n",
              "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
              "\u001b[1mTotal params\u001b[0m: 40.5 K                                                                                               \n",
              "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
              "\u001b[1mModules in train mode\u001b[0m: 4                                                                                           \n",
              "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n",
              "\u001b[1mTotal FLOPs\u001b[0m: 0                                                                                                     \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 40.5 K                                                                                           \n",
              "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
              "<span style=\"font-weight: bold\">Total params</span>: 40.5 K                                                                                               \n",
              "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
              "<span style=\"font-weight: bold\">Modules in train mode</span>: 4                                                                                           \n",
              "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
              "<span style=\"font-weight: bold\">Total FLOPs</span>: 0                                                                                                     \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b859785d54a143f6bfbb506c8120bfcf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=10` reached.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Create a PyTorch Lightning trainer\n",
        "trainer = Trainer(max_epochs=10, enable_progress_bar=True)\n",
        "\n",
        "# Train the model\n",
        "trainer.fit(model, train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28n-icBllkmT"
      },
      "source": [
        "## 8. Evaluate the model\n",
        "\n",
        "Finally, evaluate your model's performance on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "dyK6iRjElkmT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167,
          "referenced_widgets": [
            "6b1b0057f3644543b1df58f738d5210c",
            "adb95668a8f04c3180fc6fd6ef9d561f"
          ]
        },
        "outputId": "c5ef3b24-b727-4346-d684-3a39622f0142"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6b1b0057f3644543b1df58f738d5210c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
              "‚îÉ\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
              "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
              "‚îÇ\u001b[36m \u001b[0m\u001b[36m      val_acc_epoch      \u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[35m \u001b[0m\u001b[35m   0.9713000059127808    \u001b[0m\u001b[35m \u001b[0m‚îÇ\n",
              "‚îÇ\u001b[36m \u001b[0m\u001b[36m     val_loss_epoch      \u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[35m \u001b[0m\u001b[35m   0.10317420959472656   \u001b[0m\u001b[35m \u001b[0m‚îÇ\n",
              "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
              "‚îÉ<span style=\"font-weight: bold\">      Validate metric      </span>‚îÉ<span style=\"font-weight: bold\">       DataLoader 0        </span>‚îÉ\n",
              "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
              "‚îÇ<span style=\"color: #008080; text-decoration-color: #008080\">       val_acc_epoch       </span>‚îÇ<span style=\"color: #800080; text-decoration-color: #800080\">    0.9713000059127808     </span>‚îÇ\n",
              "‚îÇ<span style=\"color: #008080; text-decoration-color: #008080\">      val_loss_epoch       </span>‚îÇ<span style=\"color: #800080; text-decoration-color: #800080\">    0.10317420959472656    </span>‚îÇ\n",
              "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'val_loss_epoch': 0.10317420959472656, 'val_acc_epoch': 0.9713000059127808}]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "trainer.validate(model, dataloaders=val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmVCEOw_lkmU"
      },
      "source": [
        "## 9. Model predictions\n",
        "\n",
        "Let's see how the model performs on some randomly selected images.  Are its predictions correct?  \n",
        "\n",
        "Randomly select an image from the validation dataset, in this case, the 200th image.\n",
        "\n",
        "Select a specific image from the validation dataset for examination or prediction.\n",
        "\n",
        "The variable `loc` is set to the index 200, which means we are selecting the 201st image (0-based index) from the validation dataset.\n",
        "\n",
        "```python\n",
        "loc = 200\n",
        "\n",
        "# Extract the corresponding image from the val_features array and store it in the 'val_image' variable.\n",
        "val_image = val_features[loc]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Rcn8UZ71lkmU"
      },
      "outputs": [],
      "source": [
        "loc = 200\n",
        "\n",
        "# Extract the corresponding image from the val_features array and store it in the 'val_image' variable.\n",
        "val_image = val_features[loc]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5244ZztlkmU"
      },
      "source": [
        "First, let's take a look at the shape of the image.\n",
        "\n",
        "* Get and display the shape (dimensions) of the `val_image` variable.\n",
        "* This provides insight into the structure and size of the image.\n",
        "\n",
        "```python\n",
        "val_image.shape\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ESDbDJkOlkmU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18e8b354-313a-40ce-ec41-6eeed52b5f37"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "val_image.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPIxZ9C6lkmU"
      },
      "source": [
        "We see that our image is 28x28 pixels. However, the model needs not just the size of the image but also the batch dimension. A simple call to the `reshape()` method or `unsqueeze()` fixes that problem.\n",
        "\n",
        "* Reshape the `val_image` from a 2D array (28x28) to a 3D array (1x28x28).\n",
        "* This is commonly done to match the input shape that the model expects when making predictions on single samples.\n",
        "\n",
        "```python\n",
        "val_image_tensor = torch.tensor(val_image, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "4V08mZNUlkmU"
      },
      "outputs": [],
      "source": [
        "val_image_tensor = torch.tensor(val_image, dtype=torch.float32).unsqueeze(0)  # Add batch dimension"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5YqpAHIlkmU"
      },
      "source": [
        "Now call the model's forward pass to make a prediction, assign the output to result, and then view its contents.\n",
        "\n",
        "* Use the trained model to predict the label for the `val_image_tensor`.\n",
        "* The model returns log probabilities, so we'll convert them to probabilities and display both.\n",
        "* Each value in the array corresponds to the model's predicted probability that the image belongs to a particular class (digit).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "_jBcm-d0lkmU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef8356e2-9836-4765-eb4e-86a99cb1d64d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log probabilities: tensor([[-2267.3921, -1683.1775, -1078.7267,     0.0000, -2925.6943, -1456.7458,\n",
            "         -2866.5293, -1614.1539, -1106.5795, -1952.6298]])\n",
            "Probabilities: tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    result = model(val_image_tensor)\n",
        "    probabilities = torch.exp(result)  # Convert log probabilities to probabilities\n",
        "\n",
        "# Print the array of probabilities to the console.\n",
        "print(\"Log probabilities:\", result)\n",
        "print(\"Probabilities:\", probabilities)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lu61F5xlkmU"
      },
      "source": [
        "As we see, the model has returned the probability of 10 predictions, with the highest one being the most likely.  Use the `argmax` function to see the model's prediction.\n",
        "\n",
        "* Use the `argmax` method to find the index (label) of the maximum value in the `result` tensor.\n",
        "   * This gives us the model's most likely prediction for the class (digit) of the `val_image`.\n",
        "\n",
        "```python\n",
        "predicted_digit = result.argmax(dim=1).item()\n",
        "print(f\"Predicted digit: {predicted_digit}\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "nCX-H4hNlkmU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fb868b3-473b-4acc-ef78-734ba555d74e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted digit: 3\n"
          ]
        }
      ],
      "source": [
        "predicted_digit = result.argmax(dim=1).item()\n",
        "print(f\"Predicted digit: {predicted_digit}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tuS21VklkmU"
      },
      "source": [
        "To verify the prediction, check the label of the corresponding image.\n",
        "\n",
        "* Using the index loc, retrieve the true label (actual digit) for the `val_image` from the `val_labels` array.\n",
        "   * This gives us the actual class (digit) of the `val_image` to compare with the model's prediction.\n",
        "\n",
        "```python\n",
        "true_digit = val_labels[loc]\n",
        "print(f\"True digit: {true_digit}\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "DLBkmJ09lkmU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9c1c904-48b7-416a-cde0-ffcf1ff8253a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True digit: 3\n"
          ]
        }
      ],
      "source": [
        "true_digit = val_labels[loc]\n",
        "print(f\"True digit: {true_digit}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q56ioyAclkmU"
      },
      "source": [
        "Finally, visualize the image with pyplot.\n",
        "\n",
        "* Use the `imshow` function from the `matplotlib` library to display the `val_image` as a visual image.\n",
        "   * This helps in visually examining the content of the `val_image` (which is represented as a 28x28 array of pixel values).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "mdb1JtW0lkmU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "outputId": "821eda86-5a62-4f28-8c19-39b5a649f145"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGzCAYAAABpdMNsAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAJfdJREFUeJzt3X9wVfWd//HXDZBLCMkNIb8FQgjyY+WHI0o2A1KUlADKCMhqLNsFpys/DCiy1jaOEq3aVNvZRW2K2x03tCtoy1h0YDAWIgmrCygIRaxmkkyQMJDwY8sNJPxMPt8/+HLXaxLIudybTxKej5nPDPec877nfU4OeeWce3LiMsYYAQDQwcJsNwAAuDERQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQOhSBg8erAULFvhel5aWyuVyqbS01FpP3/XdHgG0jgBCu61Zs0Yul8s3evfurWHDhmnp0qWqq6uz3Z4jmzdv1nPPPWe7jRaOHDmif/zHf9Tw4cMVFRWlmJgYjR8/Xr/73e8UyFOzFixY4Pc1a2t01sDcsGGDsrOzlZKSIrfbrQEDBmju3Lk6cOCA7dYQBD1tN4Cu52c/+5nS0tJ07tw5ffzxx1q9erU2b96sAwcOqE+fPh3ay6RJk3T27FmFh4c7qtu8ebMKCws7XQidOHFChw8f1ty5czVo0CBdvHhRW7Zs0YIFC1ReXq6f//znjt5v0aJFysrK8r2urq7WypUrtXDhQt15552+6enp6UHbhmD64osv1K9fPz3++OOKi4tTbW2t/vM//1Pjx4/Xjh07NHbsWNst4noYoJ2KioqMJPPZZ5/5TV+xYoWRZNatW9dm7ZkzZ4LSQ2pqqpk/f/51v09ubq4J1eEfrB6/7d577zWRkZHm0qVL1/U+n332mZFkioqKrrpcsL5eoVBbW2t69uxpFi1aZLsVXCcuweG63X333ZIu/3QtXb7s07dvX1VVVWnGjBmKiorSvHnzJEnNzc1atWqVbrnlFvXu3VuJiYlatGiR/va3v/m9pzFGL774ogYMGKA+ffrorrvu0pdfftli3W19BrRr1y7NmDFD/fr1U2RkpMaMGaNXX33V119hYaEk+V2GuiLYPUpSVVWVqqqq2rtLWxg8eLAaGxt14cKFgN+jLVcurZaVlenRRx9VQkKCBgwYIOnyvho8eHCLmueee85vn13x1ltvady4cYqIiFBsbKxycnJUU1Pjt0xjY6O+/vprnThxIqB+ExIS1KdPH506dSqgenQeXILDdbvyjbV///6+aZcuXVJ2drYmTpyoX/3qV75Lc4sWLdKaNWv08MMP67HHHlN1dbV+/etfa+/evfrkk0/Uq1cvSdLKlSv14osvasaMGZoxY4Y+//xzTZ06tV3fgLds2aJ7771XycnJevzxx5WUlKSvvvpKmzZt0uOPP65FixbpyJEj2rJli/7rv/6rRX0oepwyZYok6eDBg+3ap2fPnlVDQ4POnDmjsrIyFRUVKTMzUxEREe2qD8Sjjz6q+Ph4rVy5Ug0NDY7rX3rpJT377LN64IEH9M///M86fvy4Xn/9dU2aNEl79+5VTEyMJOnTTz/VXXfdpfz8/HZfAj116pQuXryo2tparVq1SvX19b59ii7M9ikYuo4rl+C2bt1qjh8/bmpqasw777xj+vfvbyIiIszhw4eNMcbMnz/fSDI//elP/er/+7//20gya9eu9ZteXFzsN/3YsWMmPDzc3HPPPaa5udm33NNPP20k+V3e2rZtm5Fktm3bZowx5tKlSyYtLc2kpqaav/3tb37r+fZ7tXUJLhQ9GnP5slxqamqL9bWloKDASPKNKVOmmEOHDrW7vi2tXYK78nWdOHFii0t88+fPb7Xv/Px8v/138OBB06NHD/PSSy/5LffFF1+Ynj17+k2/8jXLz89vd9/Dhw/37Yu+ffuaZ555xjQ1NbW7Hp0Tl+DgWFZWluLj4zVw4EDl5OSob9++2rBhg2666Sa/5ZYsWeL3ev369fJ4PPr+97+vEydO+Ma4cePUt29fbdu2TZK0detWXbhwQcuWLfO7zLN8+fJr9rZ3715VV1dr+fLlvp+4r2jtktF3harHgwcPtvvsR5IeeughbdmyRevWrdMPfvADSZfPikLpkUceUY8ePQKq/dOf/qTm5mY98MADfvstKSlJN998s2+/SdLkyZNljHF0A0hRUZGKi4v1m9/8RiNHjtTZs2fV1NQUUK/oPLgEB8cKCws1bNgw9ezZU4mJiRo+fLjCwvx/lunZs6fvc4QrKioq5PV6lZCQ0Or7Hjt2TJL0zTffSJJuvvlmv/nx8fHq16/fVXu7cjlw1KhR7d+gDu6xPVJTU5WamirpchgtXLhQWVlZKi8vD9lluLS0tIBrKyoqZIxpsT+uuHLZMlCZmZm+f+fk5GjkyJGSpF/96lfX9b6wiwCCY+PHj9ftt99+1WXcbneLUGpublZCQoLWrl3bak18fHzQegxUZ+1x7ty5+o//+A9t375d2dnZIVlHa8HW1lnjd88+mpub5XK59MEHH7R6FtW3b9/gNCmpX79+uvvuu7V27VoCqIsjgNBh0tPTtXXrVk2YMOGqP8Vf+cm/oqJCQ4YM8U0/fvx4izvRWluHJB04cMDv91++q61vrB3RYyCuXH7zer1Bf++r6devX6t3m105A7wiPT1dxhilpaVp2LBhIe/r7NmzHb4vEHx8BoQO88ADD6ipqUkvvPBCi3mXLl3yfaPLyspSr1699Prrr/v99v+qVauuuY7bbrtNaWlpWrVqVYtvnN9+r8jISElqsUyoemzvbdjHjx9vdfqbb74pl8ul22677ZrvEUzp6enyer3av3+/b9rRo0e1YcMGv+XmzJmjHj166Pnnn2/xxAZjjE6ePOl77eQ27CuXPL/t4MGDKikpueZZODo/zoDQYb73ve9p0aJFKigo0L59+zR16lT16tVLFRUVWr9+vV599VXNnTtX8fHxevLJJ1VQUKB7771XM2bM0N69e/XBBx8oLi7uqusICwvT6tWrNXPmTN166616+OGHlZycrK+//lpffvmlPvzwQ0nSuHHjJEmPPfaYsrOz1aNHD+Xk5ISsx/behv3SSy/pk08+0bRp0zRo0CD97//+r95991199tlnWrZsmYYOHepbtrS01PHtzE7l5OToJz/5iWbPnq3HHntMjY2NWr16tYYNG6bPP//ct1x6erpefPFF5eXl6eDBg5o1a5aioqJUXV2tDRs2aOHChXryySclObsNe/To0ZoyZYpuvfVW9evXTxUVFXrzzTd18eJF/eIXvwjJNqMD2bsBD11NW09C+K758+ebyMjINuf/9re/NePGjTMREREmKirKjB492jz11FPmyJEjvmWamprM888/b5KTk01ERISZPHmyOXDgQIunDHz3NuwrPv74Y/P973/fREVFmcjISDNmzBjz+uuv++ZfunTJLFu2zMTHxxuXy9Xiluxg9mhM+2/D/vOf/2zuvfdek5KSYnr16mWioqLMhAkTTFFRkd/t3sYYs3HjRiPJvPHGG9d83yuudht2W1/XP//5z2bUqFEmPDzcDB8+3Lz11lstbsO+4t133zUTJ040kZGRJjIy0owYMcLk5uaa8vJy3zJObsPOz883t99+u+nXr5/p2bOnSUlJMTk5OWb//v3t3mZ0Xi5jAnjCIQDrnnrqKb399tuqrKyU2+223Q7gGJ8BAV3Utm3b9OyzzxI+6LI4AwIAWMEZEADACgIIAGAFAQQAsIIAAgBY0el+EbW5uVlHjhxRVFRUu55eDADoXIwxOn36tFJSUlo8E/LbOl0AHTlyRAMHDrTdBgDgOtXU1LR4Kv63dbpLcFFRUbZbAAAEwbW+n4csgAoLCzV48GD17t1bGRkZ+vTTT9tVx2U3AOgervX9PCQB9Ic//EErVqxQfn6+Pv/8c40dO1bZ2dmtPtkWAHCDCsUD5saPH29yc3N9r5uamkxKSoopKCi4Zq3X6/X97XcGg8FgdN3h9Xqv+v0+6GdAFy5c0J49e/z+GFhYWJiysrK0Y8eOFsufP39e9fX1fgMA0P0FPYBOnDihpqYmJSYm+k1PTExUbW1ti+ULCgrk8Xh8gzvgAODGYP0uuLy8PHm9Xt+oqamx3RIAoAME/feA4uLi1KNHD9XV1flNr6urU1JSUovl3W43j5MHgBtQ0M+AwsPDNW7cOJWUlPimNTc3q6SkRJmZmcFeHQCgiwrJkxBWrFih+fPn6/bbb9f48eO1atUqNTQ06OGHHw7F6gAAXVBIAujBBx/U8ePHtXLlStXW1urWW29VcXFxixsTAAA3rk73F1Hr6+vl8XhstwEAuE5er1fR0dFtzrd+FxwA4MZEAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYEVP2w3gxnLfffc5rhk0aJDjmtdee81xjSQ1NzcHVNcRwsKc/7zYkdvz7rvvOq4pLCx0XFNWVua4Bp0TZ0AAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUPI0XAlixZ4rjm5ZdfdlzTp08fxzWBPoTTGBNQXUcIZJs6cnvmzJnjuCY8PNxxzWeffea4prGx0XENQo8zIACAFQQQAMCKoAfQc889J5fL5TdGjBgR7NUAALq4kHwGdMstt2jr1q3/t5KefNQEAPAXkmTo2bOnkpKSQvHWAIBuIiSfAVVUVCglJUVDhgzRvHnzdOjQoTaXPX/+vOrr6/0GAKD7C3oAZWRkaM2aNSouLtbq1atVXV2tO++8U6dPn251+YKCAnk8Ht8YOHBgsFsCAHRCQQ+g6dOn6x/+4R80ZswYZWdna/PmzTp16pT++Mc/trp8Xl6evF6vb9TU1AS7JQBAJxTyuwNiYmI0bNgwVVZWtjrf7XbL7XaHug0AQCcT8t8DOnPmjKqqqpScnBzqVQEAupCgB9CTTz6psrIyHTx4UP/zP/+j2bNnq0ePHnrooYeCvSoAQBcW9Etwhw8f1kMPPaSTJ08qPj5eEydO1M6dOxUfHx/sVQEAujCX6WRPX6yvr5fH47HdBtqhvLzccU16enoIOmnJ5XIFVNfJ/jv4CWSbOvP2SIFt07BhwxzXVFVVOa7B9fN6vYqOjm5zPs+CAwBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArQv4H6QAgmObOneu45uWXXw5BJ7henAEBAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACp6GjYCVlZU5rklPTw9BJ7iRTJw40XENT8PunDgDAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAAreBgpAvbb3/7WcU1ycnIIOmlp+fLlHbKeQD3zzDOOa/7pn/4pBJ10PV999ZXtFhAknAEBAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUuY4yx3cS31dfXy+Px2G4DXVxMTExAdXFxcY5rFi9e7Lhm9uzZjmsGDx7suKaT/fduYePGjY5r5s2b57imsbHRcQ2un9frVXR0dJvzOQMCAFhBAAEArHAcQNu3b9fMmTOVkpIil8ul9957z2++MUYrV65UcnKyIiIilJWVpYqKimD1CwDoJhwHUENDg8aOHavCwsJW57/yyit67bXX9MYbb2jXrl2KjIxUdna2zp07d93NAgC6D8d/EXX69OmaPn16q/OMMVq1apWeeeYZ3XfffZKk3//+90pMTNR7772nnJyc6+sWANBtBPUzoOrqatXW1iorK8s3zePxKCMjQzt27Gi15vz586qvr/cbAIDuL6gBVFtbK0lKTEz0m56YmOib910FBQXyeDy+MXDgwGC2BADopKzfBZeXlyev1+sbNTU1tlsCAHSAoAZQUlKSJKmurs5vel1dnW/ed7ndbkVHR/sNAED3F9QASktLU1JSkkpKSnzT6uvrtWvXLmVmZgZzVQCALs7xXXBnzpxRZWWl73V1dbX27dun2NhYDRo0SMuXL9eLL76om2++WWlpaXr22WeVkpKiWbNmBbNvAEAX5ziAdu/erbvuusv3esWKFZKk+fPna82aNXrqqafU0NCghQsX6tSpU5o4caKKi4vVu3fv4HUNAOjyeBgpuqVt27YFVHfnnXcGuZPgcblcjms62X/vFn796187rlm+fHnwG0FI8DBSAECnRAABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBWO/xwDcD02b97suCY7O9txTVhYYD9bNTc3B1TXEQLZps68PVJgT/hG98EZEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYwcNIEbD4+HjHNf3793dcY4xxXBPoQzgDWVdHCWSbOvP2SNIPf/hDxzUffPCB45ri4mLHNQg9zoAAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAVBBAAwAoeRgrAGo/H47imqKjIcc3MmTMd10jS7t27A6pD+3AGBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABW8DBSBOz48eOOa06cOBGCTm4M27dvd1wzcuTIgNYVFxcXUF1HiI+Pd1zTv3//EHSC68UZEADACgIIAGCF4wDavn27Zs6cqZSUFLlcLr333nt+8xcsWCCXy+U3pk2bFqx+AQDdhOMAamho0NixY1VYWNjmMtOmTdPRo0d94+23376uJgEA3Y/jmxCmT5+u6dOnX3UZt9utpKSkgJsCAHR/IfkMqLS0VAkJCRo+fLiWLFmikydPtrns+fPnVV9f7zcAAN1f0ANo2rRp+v3vf6+SkhK9/PLLKisr0/Tp09XU1NTq8gUFBfJ4PL4xcODAYLcEAOiEgv57QDk5Ob5/jx49WmPGjFF6erpKS0s1ZcqUFsvn5eVpxYoVvtf19fWEEADcAEJ+G/aQIUMUFxenysrKVue73W5FR0f7DQBA9xfyADp8+LBOnjyp5OTkUK8KANCFOL4Ed+bMGb+zmerqau3bt0+xsbGKjY3V888/r/vvv19JSUmqqqrSU089paFDhyo7OzuojQMAujbHAbR7927dddddvtdXPr+ZP3++Vq9erf379+t3v/udTp06pZSUFE2dOlUvvPCC3G538LoGAHR5jgNo8uTJMsa0Of/DDz+8roa6q8GDBzuumTlzpuOaQB5Y+Ze//MVxTaBcLleH1ISFBXZ1ubq62nFNUVGR45oXXnjBcU0grvU7e23ZtGlTkDtpXSBfp+bmZsc1gRxDCD2eBQcAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArgv4nudG6tWvXOq7JyMhwXHPixAnHNXfffbfjGkn661//6rjmpz/9qeOapqYmxzWBysvLc1xz4MCBEHTSUiBPVP/5z38e0Lqu9sT7YArkydaB9NZR2wNnOAMCAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACt4GGkHCeShi4GIi4tzXLNx48aA1jVv3jzHNYE8LPWJJ55wXNORhg4d6rhm7ty5jmsC2d8jR450XAN0FM6AAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKlzHG2G7i2+rr6+XxeGy3EXSpqamOazZt2uS4hodPXuZyuQKq62T/HfwEsk2deXukwLbpyy+/dFwzc+ZMxzWS9M033wRUh8u8Xq+io6PbnM8ZEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBY0dN2AzeKQB5qWFNT47jm7/7u7xzXdEdhYYH9bNXc3BzkToInkG3qzNsjSV988YXjmqysLMc1J06ccFyD0OMMCABgBQEEALDCUQAVFBTojjvuUFRUlBISEjRr1iyVl5f7LXPu3Dnl5uaqf//+6tu3r+6//37V1dUFtWkAQNfnKIDKysqUm5urnTt3asuWLbp48aKmTp2qhoYG3zJPPPGENm7cqPXr16usrExHjhzRnDlzgt44AKBrc3QTQnFxsd/rNWvWKCEhQXv27NGkSZPk9Xr15ptvat26dbr77rslSUVFRRo5cqR27typv//7vw9e5wCALu26PgPyer2SpNjYWEnSnj17dPHiRb+7VEaMGKFBgwZpx44drb7H+fPnVV9f7zcAAN1fwAHU3Nys5cuXa8KECRo1apQkqba2VuHh4YqJifFbNjExUbW1ta2+T0FBgTwej28MHDgw0JYAAF1IwAGUm5urAwcO6J133rmuBvLy8uT1en0jkN99AQB0PQH9IurSpUu1adMmbd++XQMGDPBNT0pK0oULF3Tq1Cm/s6C6ujolJSW1+l5ut1tutzuQNgAAXZijMyBjjJYuXaoNGzboo48+Ulpamt/8cePGqVevXiopKfFNKy8v16FDh5SZmRmcjgEA3YKjM6Dc3FytW7dO77//vqKionyf63g8HkVERMjj8ehHP/qRVqxYodjYWEVHR2vZsmXKzMzkDjgAgB9HAbR69WpJ0uTJk/2mFxUVacGCBZKkf/u3f1NYWJjuv/9+nT9/XtnZ2frNb34TlGYBAN2HyxhjbDfxbfX19fJ4PLbb6BQCOWv8+OOPQ9BJ1+NyuQKq62T/HfwEsk2deXuky58nO/XGG2+EoBOEgtfrVXR0dJvzeRYcAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArAjoL6KiY3zxxReOa+655x7HNZMmTXJcI0k//OEPHdekpKQEtC50rL/85S+Oa55++mnHNR9++KHjGnQfnAEBAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUuY4yx3cS31dfXy+Px2G4D7ZCamuq4ZubMmSHopKVXX301oLpO9t/Bz/LlyztsXRs3bnRc880334SgE3RlXq9X0dHRbc7nDAgAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArOBhpACAkOBhpACATokAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACkcBVFBQoDvuuENRUVFKSEjQrFmzVF5e7rfM5MmT5XK5/MbixYuD2jQAoOtzFEBlZWXKzc3Vzp07tWXLFl28eFFTp05VQ0OD33KPPPKIjh496huvvPJKUJsGAHR9PZ0sXFxc7Pd6zZo1SkhI0J49ezRp0iTf9D59+igpKSk4HQIAuqXr+gzI6/VKkmJjY/2mr127VnFxcRo1apTy8vLU2NjY5nucP39e9fX1fgMAcAMwAWpqajL33HOPmTBhgt/0f//3fzfFxcVm//795q233jI33XSTmT17dpvvk5+fbyQxGAwGo5sNr9d71RwJOIAWL15sUlNTTU1NzVWXKykpMZJMZWVlq/PPnTtnvF6vb9TU1FjfaQwGg8G4/nGtAHL0GdAVS5cu1aZNm7R9+3YNGDDgqstmZGRIkiorK5Went5ivtvtltvtDqQNAEAX5iiAjDFatmyZNmzYoNLSUqWlpV2zZt++fZKk5OTkgBoEAHRPjgIoNzdX69at0/vvv6+oqCjV1tZKkjwejyIiIlRVVaV169ZpxowZ6t+/v/bv368nnnhCkyZN0pgxY0KyAQCALsrJ5z5q4zpfUVGRMcaYQ4cOmUmTJpnY2FjjdrvN0KFDzY9//ONrXgf8Nq/Xa/26JYPBYDCuf1zre7/r/wdLp1FfXy+Px2O7DQDAdfJ6vYqOjm5zPs+CAwBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBY0ekCyBhjuwUAQBBc6/t5pwug06dP224BABAE1/p+7jKd7JSjublZR44cUVRUlFwul9+8+vp6DRw4UDU1NYqOjrbUoX3sh8vYD5exHy5jP1zWGfaDMUanT59WSkqKwsLaPs/p2YE9tUtYWJgGDBhw1WWio6Nv6APsCvbDZeyHy9gPl7EfLrO9HzwezzWX6XSX4AAANwYCCABgRZcKILfbrfz8fLndbtutWMV+uIz9cBn74TL2w2VdaT90upsQAAA3hi51BgQA6D4IIACAFQQQAMAKAggAYAUBBACwossEUGFhoQYPHqzevXsrIyNDn376qe2WOtxzzz0nl8vlN0aMGGG7rZDbvn27Zs6cqZSUFLlcLr333nt+840xWrlypZKTkxUREaGsrCxVVFTYaTaErrUfFixY0OL4mDZtmp1mQ6SgoEB33HGHoqKilJCQoFmzZqm8vNxvmXPnzik3N1f9+/dX3759df/996uurs5Sx6HRnv0wefLkFsfD4sWLLXXcui4RQH/4wx+0YsUK5efn6/PPP9fYsWOVnZ2tY8eO2W6tw91yyy06evSob3z88ce2Wwq5hoYGjR07VoWFha3Of+WVV/Taa6/pjTfe0K5duxQZGans7GydO3eugzsNrWvtB0maNm2a3/Hx9ttvd2CHoVdWVqbc3Fzt3LlTW7Zs0cWLFzV16lQ1NDT4lnniiSe0ceNGrV+/XmVlZTpy5IjmzJljsevga89+kKRHHnnE73h45ZVXLHXcBtMFjB8/3uTm5vpeNzU1mZSUFFNQUGCxq46Xn59vxo4da7sNqySZDRs2+F43NzebpKQk88tf/tI37dSpU8btdpu3337bQocd47v7wRhj5s+fb+677z4r/dhy7NgxI8mUlZUZYy5/7Xv16mXWr1/vW+arr74yksyOHTtstRly390Pxhjzve99zzz++OP2mmqHTn8GdOHCBe3Zs0dZWVm+aWFhYcrKytKOHTssdmZHRUWFUlJSNGTIEM2bN0+HDh2y3ZJV1dXVqq2t9Ts+PB6PMjIybsjjo7S0VAkJCRo+fLiWLFmikydP2m4ppLxeryQpNjZWkrRnzx5dvHjR73gYMWKEBg0a1K2Ph+/uhyvWrl2ruLg4jRo1Snl5eWpsbLTRXps63dOwv+vEiRNqampSYmKi3/TExER9/fXXlrqyIyMjQ2vWrNHw4cN19OhRPf/887rzzjt14MABRUVF2W7PitraWklq9fi4Mu9GMW3aNM2ZM0dpaWmqqqrS008/renTp2vHjh3q0aOH7faCrrm5WcuXL9eECRM0atQoSZePh/DwcMXExPgt252Ph9b2gyT94Ac/UGpqqlJSUrR//3795Cc/UXl5uf70pz9Z7NZfpw8g/J/p06f7/j1mzBhlZGQoNTVVf/zjH/WjH/3IYmfoDHJycnz/Hj16tMaMGaP09HSVlpZqypQpFjsLjdzcXB04cOCG+Bz0atraDwsXLvT9e/To0UpOTtaUKVNUVVWl9PT0jm6zVZ3+ElxcXJx69OjR4i6Wuro6JSUlWeqqc4iJidGwYcNUWVlpuxVrrhwDHB8tDRkyRHFxcd3y+Fi6dKk2bdqkbdu2+f39sKSkJF24cEGnTp3yW767Hg9t7YfWZGRkSFKnOh46fQCFh4dr3LhxKikp8U1rbm5WSUmJMjMzLXZm35kzZ1RVVaXk5GTbrViTlpampKQkv+Ojvr5eu3btuuGPj8OHD+vkyZPd6vgwxmjp0qXasGGDPvroI6WlpfnNHzdunHr16uV3PJSXl+vQoUPd6ni41n5ozb59+ySpcx0Ptu+CaI933nnHuN1us2bNGvPXv/7VLFy40MTExJja2lrbrXWof/mXfzGlpaWmurrafPLJJyYrK8vExcWZY8eO2W4tpE6fPm327t1r9u7daySZf/3XfzV79+4133zzjTHGmF/84hcmJibGvP/++2b//v3mvvvuM2lpaebs2bOWOw+uq+2H06dPmyeffNLs2LHDVFdXm61bt5rbbrvN3HzzzebcuXO2Ww+aJUuWGI/HY0pLS83Ro0d9o7Gx0bfM4sWLzaBBg8xHH31kdu/ebTIzM01mZqbFroPvWvuhsrLS/OxnPzO7d+821dXV5v333zdDhgwxkyZNsty5vy4RQMYY8/rrr5tBgwaZ8PBwM378eLNz507bLXW4Bx980CQnJ5vw8HBz0003mQcffNBUVlbabivktm3bZiS1GPPnzzfGXL4V+9lnnzWJiYnG7XabKVOmmPLycrtNh8DV9kNjY6OZOnWqiY+PN7169TKpqanmkUce6XY/pLW2/ZJMUVGRb5mzZ8+aRx991PTr18/06dPHzJ492xw9etRe0yFwrf1w6NAhM2nSJBMbG2vcbrcZOnSo+fGPf2y8Xq/dxr+DvwcEALCi038GBADongggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAVBBAAwIr/ByCOHD5TN31VAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.imshow(val_features[loc], cmap=\"gray\")\n",
        "plt.title(f\"Predicted: {predicted_digit}, True: {true_digit}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXZve_APlkmU"
      },
      "source": [
        "And we did it! We helped Amelia create a model that can recognize handwritten digits!\n",
        "\n",
        "\n",
        "## Bonus exercise\n",
        "\n",
        "* Write a function that ties all these steps into one function call. The function should take an input image and print the image with the predicted digit and true digit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSDPyylmlkmU",
        "tags": []
      },
      "source": [
        "## Before continuing\n",
        "###  <img src='https://github.com/PracticumAI/deep_learning_pt-lightning/blob/main/images/alert_icon.svg?raw=1' alt=\"Alert icon\" width=40 align=center> Alert!\n",
        "> Before continuing to another notebook within the same Jupyter session,\n",
        "> use the **\"Running Terminals and Kernels\" tab** (below the File Browser tab) to **shut down this kernel**.\n",
        "> This will free up this notebook's GPU memory, making it available for\n",
        "> your next notebook.\n",
        ">\n",
        "> Every time you run multiple notebooks within a Jupyter session with a GPU, this should be done.\n",
        ">\n",
        "> ![Screenshot of the Running Terminals and Kernels tab used t oshut down kernels before starting a new notebook](https://github.com/PracticumAI/deep_learning_pt-lightning/blob/main/images/stop_kernel.png?raw=1)\n",
        "\n",
        "----\n",
        "## Push changes to GitHub <img src=\"https://github.com/PracticumAI/deep_learning_pt-lightning/blob/main/images/push_to_github.png?raw=1\" alt=\"Push to GitHub icon\" align=\"right\" width=150>\n",
        "\n",
        " Remember to **add**, **commit**, and **push** the changes you have made to this notebook to GitHub to keep your repository in sync.\n",
        "\n",
        "In Jupyter, those are done in the git tab on the left. In Google Colab, use File > Save a copy in GitHub.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dlf_workshop",
      "language": "python",
      "name": "dlf_workshop"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b859785d54a143f6bfbb506c8120bfcf": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_be127b5393ec48d6b4c4f3e0b1e96751",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "Epoch 9/9  \u001b[38;2;98;6;224m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m 938/938 \u001b[2m0:00:20 ‚Ä¢ 0:00:00\u001b[0m \u001b[2;4m45.40it/s\u001b[0m \u001b[3mv_num: 0.000 train_loss_step:     \u001b[0m\n                                                                                 \u001b[3m0.010 train_acc_step: 1.000       \u001b[0m\n                                                                                 \u001b[3mval_loss_step: 0.000 val_acc_step:\u001b[0m\n                                                                                 \u001b[3m1.000 val_loss_epoch: 0.102       \u001b[0m\n                                                                                 \u001b[3mval_acc_epoch: 0.970              \u001b[0m\n                                                                                 \u001b[3mtrain_loss_epoch: 0.053           \u001b[0m\n                                                                                 \u001b[3mtrain_acc_epoch: 0.983            \u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Epoch 9/9  <span style=\"color: #6206e0; text-decoration-color: #6206e0\">‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ</span> 938/938 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">0:00:20 ‚Ä¢ 0:00:00</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; text-decoration: underline\">45.40it/s</span> <span style=\"font-style: italic\">v_num: 0.000 train_loss_step:     </span>\n                                                                                 <span style=\"font-style: italic\">0.010 train_acc_step: 1.000       </span>\n                                                                                 <span style=\"font-style: italic\">val_loss_step: 0.000 val_acc_step:</span>\n                                                                                 <span style=\"font-style: italic\">1.000 val_loss_epoch: 0.102       </span>\n                                                                                 <span style=\"font-style: italic\">val_acc_epoch: 0.970              </span>\n                                                                                 <span style=\"font-style: italic\">train_loss_epoch: 0.053           </span>\n                                                                                 <span style=\"font-style: italic\">train_acc_epoch: 0.983            </span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "be127b5393ec48d6b4c4f3e0b1e96751": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b1b0057f3644543b1df58f738d5210c": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_adb95668a8f04c3180fc6fd6ef9d561f",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "Validation \u001b[38;2;98;6;224m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m 157/157 \u001b[2m0:00:02 ‚Ä¢ 0:00:00\u001b[0m \u001b[2;4m55.41it/s\u001b[0m  \n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Validation <span style=\"color: #6206e0; text-decoration-color: #6206e0\">‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ</span> 157/157 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">0:00:02 ‚Ä¢ 0:00:00</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; text-decoration: underline\">55.41it/s</span>  \n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "adb95668a8f04c3180fc6fd6ef9d561f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}